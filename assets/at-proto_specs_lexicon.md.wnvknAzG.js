import{_ as t,o as i,c as o,ag as a}from"./chunks/framework.BSbZkL3e.js";const u=JSON.parse('{"title":"preferred","description":"","frontmatter":{},"headers":[],"relativePath":"at-proto/specs/lexicon.md","filePath":"at-proto/specs/lexicon.md"}'),n={name:"at-proto/specs/lexicon.md"};function s(r,e,d,l,c,p){return i(),o("div",null,[...e[0]||(e[0]=[a('<p>Lexicon Lexicon is a schema definition language used to describe atproto records, HTTP endpoints (XRPC), and event stream messages. It builds on top of the atproto Data Model.</p><p>The schema language is similar to JSON Schema and OpenAPI, but includes some atproto-specific features and semantics.</p><p>This specification describes version 1 of the Lexicon definition language.</p><p>Overview of Types Lexicon Type Data Model Type Category boolean Boolean concrete integer Integer concrete string String concrete bytes Bytes concrete cid-link Link concrete blob Blob concrete array Array container object Object container params sub-type permission sub-type token meta ref meta union meta unknown meta record primary query primary procedure primary subscription primary permission-set primary Lexicon Files Lexicons are JSON files associated with a single NSID. A file contains one or more definitions, each with a distinct short name. A definition with the name main optionally describes the &quot;primary&quot; definition for the entire file. A Lexicon with zero definitions is invalid.</p><p>A Lexicon JSON file is an object with the following fields:</p><p>lexicon (integer, required): indicates Lexicon language version. In this version, a fixed value of 1 id (string, required): the NSID of the Lexicon description (string, optional): short overview of the Lexicon, usually one or two sentences defs (map of strings-to-objects, required): set of definitions, each with a distinct name (key) Schema definitions under defs all have a type field to distinguish their type. A file can have at most one definition with one of the &quot;primary&quot; types. Primary types should always have the name main. It is possible for main to describe a non-primary type. Not all types can be directly used as a definition. For example, sub-types, unknown, ref, and union can not be used as named definitions.</p><p>References to specific definitions within a Lexicon use fragment syntax, like com.example.defs#someView. If a main definition exists, it can be referenced without a fragment, just using the NSID. For references in the $type fields in data objects themselves (eg, records or contents of a union), this is a &quot;must&quot; (use of a #main suffix is invalid). For example, com.example.record not com.example.record#main.</p><p>Related Lexicons are often grouped together in the NSID hierarchy. As a convention, any definitions used by multiple Lexicons are defined in a dedicated _.defs Lexicon (eg, com.atproto.server.defs) within the group. A _.defs Lexicon should generally not include a definition named main, though it is not strictly invalid to do so.</p><p>Primary Type Definitions The primary types are:</p><p>query: describes an XRPC Query (HTTP GET) procedure: describes an XRPC Procedure (HTTP POST) subscription: Event Stream (WebSocket) record: describes an object that can be stored in a repository record permission-set: describes a bundle of auth permissions All of the primary definition types include these fields:</p><p>type (string, required): the type value (eg, record for records) description (string, optional): short, usually only a sentence or two Record Specifies schema of data objects stored in Repositories.</p><p>Type-specific fields:</p><p>key (string, required): specifies the Record Key type record (object, required): a schema definition with type object, which specifies this type of record Query and Procedure (HTTP API) Specifies an XRPC HTTP API endpoint.</p><p>Type-specific fields:</p><p>parameters (object, optional): a schema definition with type params, describing the HTTP query parameters for this endpoint output (object, optional): describes the HTTP response body description (string, optional): short description encoding (string, required): MIME type for body contents. Use application/json for JSON responses. schema (object, optional): schema definition, either an object, a ref, or a union of refs. Used to describe JSON encoded responses, though schema is optional even for JSON responses. input (object, optional, exclusive to procedure): describes HTTP request body schema, with the same format as the output field errors (array of objects, optional): set of string error codes which might be returned name (string, required): short name for the error type, with no whitespace description (string, optional): short description, one or two sentences Subscription (Event Stream) Specifies an Event Stream endpoint, eg messages on a WebSocket.</p><p>Type-specific fields:</p><p>parameters (object, optional): same as Query and Procedure message (object, required): specifies what messages can be description (string, optional): short description schema (object, required): schema definition, which must be a union of refs errors (array of objects, optional): same as Query and Procedure Subscription schemas (referenced by the schema field under message) must be a union of refs, not an object type.</p><p>Permission Set Specifies a bundle of permissions for use as OAuth scopes. See Permissions specification for more details.</p><p>Type-specific fields:</p><p>title (string, optional): short name for the permission set, which will be displayed to users. Should be limited to a handful of words title:lang (map of strings to strings, optional): internationalized/localized variations of title for display. The map key strings must be valid language codes; see the language lexicon string format detail (string, optional): human-meaningful description of the scope of this permission set, which will be displayed to users. Should be limited to a paragraph or so. detail:lang (map of strings to strings, optional): same as title:lang, but for detail permissions (array of permission definitions, required): the permissions included in this set Field Type Definitions As with the primary definitions, every schema object includes these fields:</p><p>type (string, required): fixed value for each type description (string, optional): short, usually only a sentence or two boolean Type-specific fields:</p><p>default (boolean, optional): a default value for this field const (boolean, optional): a fixed (constant) value for this field When included as an HTTP query parameter, should be rendered as true or false (no quotes).</p><p>integer A signed integer number.</p><p>Type-specific fields:</p><p>minimum (integer, optional): minimum acceptable value maximum (integer, optional): maximum acceptable value enum (array of integers, optional): a closed set of allowed values default (integer, optional): a default value for this field const (integer, optional): a fixed (constant) value for this field string Type-specific fields:</p><p>format (string, optional): string format restriction maxLength (integer, optional): maximum length of value, in UTF-8 bytes minLength (integer, optional): minimum length of value, in UTF-8 bytes maxGraphemes (integer, optional): maximum length of value, counted as Unicode Grapheme Clusters minGraphemes (integer, optional): minimum length of value, counted as Unicode Grapheme Clusters knownValues (array of strings, optional): a set of suggested or common values for this field. Values are not limited to this set (aka, not a closed enum). enum (array of strings, optional): a closed set of allowed values default (string, optional): a default value for this field const (string, optional): a fixed (constant) value for this field Strings are Unicode. For non-Unicode encodings, use bytes instead. The basic minLength/maxLength validation constraints are counted as UTF-8 bytes. Note that Javascript stores strings with UTF-16 by default, and it is necessary to re-encode to count accurately. The minGraphemes/maxGraphemes validation constraints work with Grapheme Clusters, which have a complex technical and linguistic definition, but loosely correspond to &quot;distinct visual characters&quot; like Latin letters, CJK characters, punctuation, digits, or emoji (which might comprise multiple Unicode codepoints and many UTF-8 bytes).</p><p>format constrains the string format and provides additional semantic context. Refer to the Data Model specification for the available format types and their definitions.</p><p>const and default are mutually exclusive.</p><p>bytes Type-specific fields:</p><p>minLength (integer, optional): minimum size of value, as raw bytes with no encoding maxLength (integer, optional): maximum size of value, as raw bytes with no encoding In the JSON format, these fields are encoded as {&quot;$bytes&quot;:<code>&lt;base64-string&gt;</code>}. In the CBOR format, these fields use the native bytes type.</p><p>cid-link No type-specific fields.</p><p>See Data Model spec for CID restrictions.</p><p>array Type-specific fields:</p><p>items (object, required): describes the schema elements of this array minLength (integer, optional): minimum count of elements in array maxLength (integer, optional): maximum count of elements in array In theory arrays have homogeneous types (meaning every element as the same type). However, with union types this restriction is meaningless, so implementations can not assume that all the elements have the same type.</p><p>object A generic object schema which can be nested inside other definitions by reference.</p><p>Type-specific fields:</p><p>properties (map of strings-to-objects, required): defines the properties (fields) by name, each with their own schema required (array of strings, optional): indicates which properties are required nullable (array of strings, optional): indicates which properties can have null as a value As described in the data model specification, there is a semantic difference in data between omitting a field; including the field with the value null; and including the field with a &quot;false-y&quot; value (false, 0, empty array, etc).</p><p>blob Type-specific fields:</p><p>accept (array of strings, optional): list of acceptable MIME types. Each may end in _ as a glob pattern (eg, image/_). Use <em>/</em> to indicate that any MIME type is accepted. Partial globs (text/ht*) or stand-alone wildcard (*) are not supported. maxSize (integer, optional): maximum size in bytes params This is a limited-scope type which is only ever used for the parameters field on query, procedure, and subscription primary types. params map to HTTP query parameters. They can not appear in other contexts.</p><p>Type-specific fields:</p><p>required (array of strings, optional): same semantics as field on object properties: similar to properties under object, but can only include the types boolean, integer, string; or an array of one of these types Note that unlike object, there is no nullable field on params.</p><p>params can not be declared as a top-level named type in a schema defs array. This means that they can not be referenced by a ref or union.</p><p>permission This is a limited-scope type which is only ever used for the permissions field of the permission-set primary type, and should not appear in other schema contexts. They map to auth permissions.</p><p>Type-specific fields:</p><p>resource (string, required): indicates the resource type which access is being granted to Additional permission fields depend on the resource type being declared. The set of resources and fields may expand over time, but currently includes:</p><p>repo: public repository write permissions collection (array of unique strings, required): indicates record types. Must include at least one element (not empty array). Does not support wildcard (<em>) in Lexicon schema context. action (array of unique strings, optional): limits set of record operations. Values must be one or create, update, delete, case-sensitive. rpc: remote API calls via proxying or service token generation lxm (array of strings, required): allowed XRPC endpoints, as NSIDs. Must include at least one element (not empty array). Wildcard (</em>) is not supported in Lexicon schema context. aud (string, semi-required): remote service (&quot;audience&quot;). In the context of permission sets, can not have fixed DID service references. Supports wildcard (*), though aud and lxm can not both be wildcard. Required unless inheritAud is true. Effectively must be either wildcard, or undefined with inheritAud set to true. inheritAud (boolean, optional): if true, aud does not need to be defined. Note that the blob, account, and identity resources types are not supported in Lexicon schema context, and should be ignored if included.</p><p>Permission declarations with unsupported resource types or parameters (fields) must be ignored by services implementing access control. This is because the set of permissions is expected to evolve over time, and additional fields might attenuate the permission granted. It is unsafe to ignore unknown fields when parsing permissions for access control.</p><p>See Permission specification for more details. Note that it is possible to construct a valid lexicon permission type which is invalid or has undefined behavior as an auth permission.</p><p>token Tokens are empty data values which exist only to be referenced by name. They are used to define a set of values with specific meanings. The description field should clarify the meaning of the token. They have no type-specific fields.</p><p>Tokens do not themselves have a data representation. They can only be declared as named definitions. For example, they can not be referenced via ref or union, or included as a field in an object.</p><p>One place token references might appear in other Lexicon definitions is as strings in the knownValues or enum fields of a string definition. Tokens should always be fully-qualified references (an NSID followed by an optional fragment), not local references within a schema file..</p><p>Tokens are similar to the concept of a &quot;symbol&quot; in some programming languages, distinct from strings, variables, built-in keywords, or other identifiers. For example, tokens could be defined to represent the state of an entity (in a state machine), or to enumerate a list of categories.</p><p>ref Type-specific fields:</p><p>ref (string, required): reference to another schema definition Refs are a mechanism for re-using a schema definition in multiple places. The intended use is to reduce duplication of definitions for types like object or a string with specific restrictions.</p><p>The ref string can be a global reference to a Lexicon type definition (an NSID, optionally with a #-delimited name indicating a definition other than main), or can indicate a local definition within the same Lexicon file (a # followed by a name).</p><p>When a ref points to an object type, the type of the encoded data is unambiguous. This means that the $type field should not be included in encoded data as a discriminator. This does not apply to references to record types, which always include the $type discriminator.</p><p>A ref can not point to a token type.</p><p>Refs can not be declared as top-level named types in a schema defs array. This means that a ref can not point to another ref, nor a union. They can reference container types that contain nested ref types, for example an object.</p><p>union Type-specific fields:</p><p>refs (array of strings, required): references to schema definitions closed (boolean, optional): indicates if a union is &quot;open&quot; or &quot;closed&quot;. defaults to false (open union) Unions represent that multiple possible types could be present at this location in the schema. The references follow the same syntax as ref, allowing references to both global or local schema definitions. Actual data will validate against a single specific type: the union does not combine fields from multiple schemas, or define a new hybrid data type. The different types are referred to as variants.</p><p>By default unions are &quot;open&quot;, meaning that future revisions of the schema could add more types to the list of refs (though can not remove types). This means that implementations should be permissive when validating, in case they do not have the most recent version of the Lexicon. The closed flag (boolean) can indicate that the set of types is fixed and can not be extended in the future.</p><p>A union schema definition with no refs is allowed and similar to unknown, as long as the closed flag is false (the default). The main difference is that the data is required to include the $type field.</p><p>A closed union with only a single type referenced is similar to ref. The main difference is that the data is required to include the $type field.</p><p>An empty closed union (a union with an empty refs list and the closed flag set to true) is an invalid schema.</p><p>The schema definitions pointed to by a union must have type object or record. All the variants must be represented by a CBOR map (or JSON Object) and must include a $type field indicating the variant type. A union can not point to types query, procedure, subscription, unknown, blob, cid-link, array, params, token, ref, union, or any other currently defined types.</p><p>Unions can not be declared as top-level named types in a schema file defs map. This means that a union can not reference another union, nor a ref.</p><p>unknown An unknown type indicates that any data object could appear at this location, with no specific validation. The top-level data must be an object (not a string, boolean, etc). unknown definitions have no type-specific fields.</p><p>As with all other data types, the value null is not allowed unless the field is specifically marked as nullable.</p><p>The data object may contain a $type field indicating the schema of the data, but it is not required. The top-level data object must not have the structure of a compound data type, like blob ($type: blob) or CID link ($link).</p><p>The (nested) contents of the data object must still be valid under the atproto data model. For example, it should not contain floats. Nested compound types like blobs and CID links should be validated and transformed as expected.</p><p>Lexicon designers are strongly recommended to not use unknown fields in record objects for now.</p><p>unknown types can not be declared as top-level named types in a schema defs array.</p><p>String Formats Strings can optionally be constrained to one of the following format types:</p><p>at-identifier: either a Handle or a DID, details described below at-uri: AT-URI cid: CID in string format, details specified in Data Model datetime: timestamp, details specified below did: generic DID Identifier handle: Handle Identifier nsid: Namespaced Identifier tid: Timestamp Identifier (TID) record-key: Record Key, matching the general syntax (&quot;any&quot;) uri: generic URI, details specified below language: language code, details specified below For the various identifier formats, when doing Lexicon schema validation the most expansive identifier syntax format should be permitted. Problems with identifiers which do pass basic syntax validation should be reported as application errors, not lexicon data validation errors. For example, data with any kind of DID in a did format string field should pass Lexicon validation, with unsupported DID methods being raised separately as an application error.</p><p>at-identifier A string type which is either a DID (type: did) or a handle (handle). Mostly used in XRPC query parameters. It is unambiguous whether an at-identifier is a handle or a DID because a DID always starts with did:, and the colon character (ðŸ˜ƒ is not allowed in handles.</p><p>datetime Full-precision date and time, with timezone information.</p><p>This format is intended for use with computer-generated timestamps in the modern computing era (eg, after the UNIX epoch). If you need to represent historical or ancient events, ambiguity, or far-future times, a different format is probably more appropriate. Datetimes before year zero (in astonomical time) are specifically disallowed.</p><p>Datetime format standards are notoriously flexible and overlapping. Datetime strings in atproto should meet the intersecting requirements of the RFC 3339, ISO 8601, and WHATWG HTML datetime standards.</p><p>The character separating &quot;date&quot; and &quot;time&quot; parts must be an upper-case T.</p><p>Timezone specification is required. It is strongly preferred to use the UTC timezone, and to represent the timezone with a simple capital Z suffix (lower-case is not allowed). While hour/minute suffix syntax (like +01:00 or -10:30) is supported, &quot;negative zero&quot; (-00:00) is specifically disallowed (by ISO 8601).</p><p>Whole seconds precision is required, and arbitrary fractional precision digits are allowed. Best practice is to use at least millisecond precision, and to pad with zeros to the generated precision (eg, trailing :12.340Z instead of :12.34Z). Not all datetime formatting libraries support trailing zero formatting. Both millisecond and microsecond precision have reasonable cross-language support; nanosecond precision does not.</p><p>Implementations should be aware when round-tripping records containing datetimes of two ambiguities: loss-of-precision, and ambiguity with trailing fractional second zeros. If de-serializing Lexicon records into native types, and then re-serializing, the string representation may not be the same, which could result in broken hash references, sanity check failures, or repository update churn. A safer thing to do is to deserialize the datetime as a simple string, which ensures round-trip re-serialization.</p><p>Implementations &quot;should&quot; validate that the semantics of the datetime are valid. For example, a month or day 00 is invalid.</p><p>Valid examples:</p><h1 id="preferred" tabindex="-1">preferred <a class="header-anchor" href="#preferred" aria-label="Permalink to &quot;preferred&quot;">â€‹</a></h1><p>1985-04-12T23:20:50.123Z 1985-04-12T23:20:50.123456Z 1985-04-12T23:20:50.120Z 1985-04-12T23:20:50.120000Z 0001-01-01T00:00:00.000Z 0000-01-01T00:00:00.000Z</p><h1 id="supported" tabindex="-1">supported <a class="header-anchor" href="#supported" aria-label="Permalink to &quot;supported&quot;">â€‹</a></h1><p>1985-04-12T23:20:50.12345678912345Z 1985-04-12T23:20:50Z 1985-04-12T23:20:50.0Z 1985-04-12T23:20:50.123+00:00 1985-04-12T23:20:50.123-07:00</p><p>Invalid examples:</p><p>1985-04-12 1985-04-12T23:20Z 1985-04-12T23:20:5Z 1985-04-12T23:20:50.123 +001985-04-12T23:20:50.123Z 23:20:50.123Z -1985-04-12T23:20:50.123Z 1985-4-12T23:20:50.123Z 01985-04-12T23:20:50.123Z 1985-04-12T23:20:50.123+00 1985-04-12T23:20:50.123+0000</p><h1 id="iso-8601-strict-capitalization" tabindex="-1">ISO-8601 strict capitalization <a class="header-anchor" href="#iso-8601-strict-capitalization" aria-label="Permalink to &quot;ISO-8601 strict capitalization&quot;">â€‹</a></h1><p>1985-04-12t23:20:50.123Z 1985-04-12T23:20:50.123z</p><h1 id="rfc-3339-but-not-iso-8601" tabindex="-1">RFC-3339, but not ISO-8601 <a class="header-anchor" href="#rfc-3339-but-not-iso-8601" aria-label="Permalink to &quot;RFC-3339, but not ISO-8601&quot;">â€‹</a></h1><p>1985-04-12T23:20:50.123-00:00 1985-04-12 23:20:50.123Z</p><h1 id="timezone-is-required" tabindex="-1">timezone is required <a class="header-anchor" href="#timezone-is-required" aria-label="Permalink to &quot;timezone is required&quot;">â€‹</a></h1><p>1985-04-12T23:20:50.123</p><h1 id="syntax-looks-ok-but-datetime-is-not-valid" tabindex="-1">syntax looks ok, but datetime is not valid <a class="header-anchor" href="#syntax-looks-ok-but-datetime-is-not-valid" aria-label="Permalink to &quot;syntax looks ok, but datetime is not valid&quot;">â€‹</a></h1><p>1985-04-12T23:99:50.123Z 1985-00-12T23:20:50.123Z</p><h1 id="iso-8601-but-normalizes-to-a-negative-time" tabindex="-1">ISO-8601, but normalizes to a negative time <a class="header-anchor" href="#iso-8601-but-normalizes-to-a-negative-time" aria-label="Permalink to &quot;ISO-8601, but normalizes to a negative time&quot;">â€‹</a></h1><p>0000-01-01T00:00:00+01:00</p><p>uri Flexible to any URI schema, following the generic RFC-3986 on URIs. This includes, but isnâ€™t limited to: did, https, wss, ipfs (for CIDs), dns, and of course at. Maximum length in Lexicons is 8 KBytes.</p><p>language An IETF Language Tag string, compliant with BCP 47, defined in RFC 5646 (&quot;Tags for Identifying Languages&quot;). This is the same standard used to identify languages in HTTP, HTML, and other web standards. The Lexicon string must validate as a &quot;well-formed&quot; language tag, as defined in the RFC. Clients should ignore language strings which are &quot;well-formed&quot; but not &quot;valid&quot; according to the RFC.</p><p>As specified in the RFC, ISO 639 two-character and three-character language codes can be used on their own, lower-cased, such as ja (Japanese) or ban (Balinese). Regional sub-tags can be added, like pt-BR (Brazilian Portuguese). Additional subtags can also be added, such as hy-Latn-IT-arevela.</p><p>Language codes generally need to be parsed, normalized, and matched semantically, not simply string-compared. For example, a search engine might simplify language tags to ISO 639 codes for indexing and filtering, while a client application (user agent) would retain the full language code for presentation (text rendering) locally.</p><p>When to use $type Data objects sometimes include a $type field which indicates their Lexicon type. The general principle is that this field needs to be included any time there could be ambiguity about the content type when validating data.</p><p>The specific rules are:</p><p>record objects must always include $type. While the type is often known from context (eg, the collection part of the path for records stored in a repository), record objects can also be passed around outside of repositories and need to be self-describing union variants must always include $type, except at the top level of subscription messages Note that blob objects always include $type, which allows generic processing.</p><p>As a reminder, main types must be referenced in $type fields as just the NSID, not including a #main suffix.</p><p>Validation Options A PDS has three Lexicon validation options when creating or updating records:</p><p>Explicit validation required: the record must be validated against a Lexicon schema. If the PDS does not know of the Lexicon, and/or can not resolve it on demand, the record creation fails. Explicit no validation: the record is not validated against any Lexicon, even if the PDS knows the Lexicon locally. The record still has to pass data validation rules (eg, no malformed blob objects at any nested depth). Optimistic validation (default if no explicit argument): if the PDS knows the record Lexicon, it validates. If the PDS does not know it, and can&#39;t resolve it (or does not do live validation), the record is allowed to be created. This is also referred to as &quot;Fail-Open&quot;. In Go, this is implemented by setting Validate to true or false on the CreateRecord or UpdateRecord options struct, or leaving it unset (nil) for optimistic validation.</p><p>In Typescript, this is implemented by setting validate to true or false on the createRecord or updateRecord options object, or leaving it unset (undefined) for optimistic validation.</p><p>A flag in the success response from the PDS will indicate whether Lexicon validation happened.</p><p>Lexicon Evolution Lexicons are allowed to change over time, within some bounds to ensure both forwards and backwards compatibility. The basic principle is that all old data must still be valid under the updated Lexicon, and new data must be valid under the old Lexicon.</p><p>Any new fields must be optional Non-optional fields can not be removed. A best practice is to retain all fields in the Lexicon and mark them as deprecated if they are no longer used. Types can not change Fields can not be renamed If larger breaking changes are necessary, a new Lexicon name must be used.</p><p>It can be ambiguous when a Lexicon has been published and becomes &quot;set in stone&quot;. At a minimum, public adoption and implementation by a third party, even without explicit permission, indicates that the Lexicon has been released and should not break compatibility. A best practice is to clearly indicate in the Lexicon type name any experimental or development status. Eg, com.corp.experimental.newRecord.</p><p>Authority and Control The authority for a Lexicon is determined by the NSID, and rooted in DNS control of the domain authority. That authority has ultimate control over the Lexicon definition, and responsibility for maintenance and distribution of Lexicon schema definitions.</p><p>In a crisis, such as unintentional loss of DNS control to a bad actor, the protocol ecosystem could decide to disregard this chain of authority. This should only be done in exceptional circumstances, and not as a mechanism to subvert an active authority. The primary mechanism for resolving protocol disputes is to fork Lexicons in to a new namespace.</p><p>Protocol implementations should generally consider data which fails to validate against the Lexicon to be entirely invalid, and should not try to repair or do partial processing on the individual piece of data.</p><p>Unexpected fields in data which otherwise conforms to the Lexicon should be ignored. When doing schema validation, they should be treated at worst as warnings. This is necessary to allow evolution of the schema by the controlling authority, and to be robust in the case of out-of-date Lexicons.</p><p>Third parties can technically insert any additional fields they want into data. This is not the recommended way to extend applications, but it is not specifically disallowed. One danger with this is that the Lexicon may be updated to include fields with the same field names but different types, which would make existing data invalid.</p><p>Lexicon Publication and Resolution Lexicon schemas are published publicly as records in atproto repositories, using the com.atproto.lexicon.schema type. The domain name authority for NSIDs to specific atproto repositories (identified by DID is linked by a DNS TXT record (_lexicon), similar to but distinct from the handle resolution system.</p><p>The com.atproto.lexicon.schema Lexicon itself is very minimal: it only requires the lexicon integer field, which must be 1 for this version of the Lexicon language. In practice, same fields as Lexicon Files should be included, along with $type. The record key is the NSID of the schema.</p><p>A summary of record fields:</p><p>$type: must be com.atproto.lexicon.schema (as with all atproto records) lexicon: integer, indicates the overall version of the Lexicon (currently 1) id: the NSID of this Lexicon. Must be a simple NSID (no fragment), and must match the record key defs: the schema definitions themselves, as a map-of-objects. Names should not include a # prefix. description: optional description of the overall schema; though descriptions are best included on individual defs, not the overall schema. The com.atproto.lexicon.schema meta-schema is somewhat unlike other Lexicons, in that it is defined and governed as part of the protocol. Future versions of the language and protocol might not follow the evolution rules. It is an intentional decision to not express the Lexicon schema language itself recursively, using the schema language.</p><p>Authority for NSID namespaces is done at the &quot;group&quot; level, meaning that all NSIDs which differ only by the final &quot;name&quot; part are all published in the same repository. Lexicon resolution of NSIDs is not hierarchical: DNS TXT records must be created for each authority section, and resolvers should not recurse up or down the DNS hierarchy looking for TXT records.</p><p>For example, an NSID app.toy.record would resolve via _lexicon.toy.app.</p><p>As a more complex example, the NSID edu.university.dept.lab.blogging.getBlogPost has a &quot;name&quot; getBlogPost. Removing the name and reversing the rest of the NSID gives an &quot;authority domain name&quot; of blogging.lab.dept.university.edu. To link the authority to a specific DID (say did:plc:ewvi7nxzyoun6zhxrhs64oiz), a DNS TXT record with the name _lexicon.blogging.lab.dept.university.edu and value did=did:plc:ewvi7nxzyoun6zhxrhs64oiz (note the did= prefix) would be created. Then a record with collection com.atproto.lexicon.schema and record-key edu.university.dept.lab.blogging.getBlogPost would be created in that account&#39;s repository.</p><p>A resolving service would start with the NSID (edu.university.dept.lab.blogging.getBlogPost) and do a DNS TXT resolution for _lexicon.blogging.lab.dept.university.edu. Finding the DID, it would proceed with atproto DID resolution, look for a PDS, and then fetch the relevant record. The overall AT-URI for the record would be at://did:plc:ewvi7nxzyoun6zhxrhs64oiz/com.atproto.lexicon.schema/edu.university.dept.lab.blogging.getBlogPost.</p><p>If the DNS TXT resolution for _lexicon.blogging.lab.dept.university.edu failed, the resolving service would NOT try _lexicon.lab.dept.university.edu or _lexicon.getBlogPost.blogging.lab.dept.university.edu or _lexicon.university.edu, or any other domain name. The Lexicon resolution would simply fail.</p><p>If another NSID edu.university.dept.lab.blogging.getBlogComments was created, it would have the same authority name, and must be published in the same atproto repository (with a different record key). If a Lexicon for edu.university.dept.lab.gallery.photo was published, a new DNS TXT record would be required (_lexicon.gallery.lab.dept.university.edu; it could point at the same repository (DID), or a different repository.</p><p>A single repository can host Lexicons for multiple authority domains, possibly across multiple registered domains and TLDs. Resolution DNS records can change over time, moving schema resolution to different repositories, though it may take time for DNS and cache changes to propagate.</p><p>Note that Lexicon record operations are broadcast over repository event streams (&quot;firehose&quot;), but that DNS resolution changes do not (unlike handle changes). Resolving services should not cache DNS resolution results for long time periods.</p><p>Usage and Implementation Guidelines It should be possible to translate Lexicon schemas to JSON Schema or OpenAPI and use tools and libraries from those ecosystems to work with atproto data in JSON format.</p><p>Implementations which serialize and deserialize data from JSON or CBOR into structures derived from specific Lexicons should be aware of the risk of &quot;clobbering&quot; unexpected fields. For example, if a Lexicon is updated to add a new (optional) field, old implementations would not be aware of that field, and might accidentally strip the data when de-serializing and then re-serializing. Depending on the context, one way to avoid this problem is to retain any &quot;extra&quot; fields, or to pass-through the original data object instead of re-serializing it.</p><p>Possible Future Changes The validation rules for unexpected additional fields may change. For example, a mechanism for Lexicons to indicate that the schema is &quot;closed&quot; and unexpected fields are not allowed, or a convention around field name prefixes (ðŸ˜† to indicate unofficial extension.</p>',135)])])}const m=t(n,[["render",s]]);export{u as __pageData,m as default};
